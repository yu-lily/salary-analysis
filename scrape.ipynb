{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Page\n",
    "driver = webdriver.Firefox()\n",
    "resopnse = driver.get('https://www.levels.fyi/comp.html?track=Data%20Scientist') \n",
    "\n",
    "#Display 100 items per page\n",
    "dropdown = driver.find_elements(By.CLASS_NAME, 'fixed-table-pagination')[0].find_elements(By.CLASS_NAME, 'btn-group')[0]\n",
    "dropdown.click()\n",
    "dropdown.find_elements(By.TAG_NAME, \"li\")[-1].find_element(By.TAG_NAME, \"a\").click()\n",
    "\n",
    "#Remove regional filter\n",
    "driver.find_elements(By.CLASS_NAME, 'remove-search-filter-region-tag')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page():\n",
    "    driver.find_element(By.CLASS_NAME, 'page-next').find_element(By.TAG_NAME, \"a\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reached_end():\n",
    "    pagination_str = driver.find_element(By.CLASS_NAME, \"pagination-detail\").text\n",
    "    _, cur, total, _ = re.findall(r'\\d+', pagination_str)\n",
    "    return cur == total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_rows():\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"#compTable\")\n",
    "    rows = table.find_element(By.CSS_SELECTOR, \"tbody\").find_elements(By.CSS_SELECTOR, \"tr[data-has-detail-view]\")\n",
    "\n",
    "    #Expand all rows\n",
    "    for row in rows:\n",
    "        row.find_element(By.CSS_SELECTOR, \"td.d-none\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse data\n",
    "def parse():\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"#compTable\")\n",
    "    rows = table.find_element(By.CSS_SELECTOR, \"tbody\").find_elements(By.CSS_SELECTOR, \"tr\")\n",
    "    #assert(len(rows) == 200)\n",
    "\n",
    "    parsed_rows = []\n",
    "    for row, details in chunks(rows, 2):\n",
    "        data = row.text.replace('|', '\\n').split('\\n')\n",
    "        data = list(map(str.strip, data))\n",
    "        data = [d for d in data if not d.startswith('+')]\n",
    "\n",
    "        other_details_section = details.find_elements(By.CSS_SELECTOR, \".other-details__content\")\n",
    "        if other_details_section:\n",
    "            other_details = other_details_section[0].text\n",
    "        else:\n",
    "            other_details = \"N/A\"\n",
    "        data.extend([other_details])\n",
    "\n",
    "        gender_section = details.find_elements(By.CSS_SELECTOR, \"p.small\")\n",
    "        if gender_section:\n",
    "            gender = gender_section[0].text\n",
    "        else:\n",
    "            gender = \"N/A\"\n",
    "        data.extend([gender])\n",
    "        \n",
    "        #assert(len(data) == len(cols))\n",
    "        parsed_rows.append(data)\n",
    "    return parsed_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output file if it doesn't exist\n",
    "cols = ['Company', 'Location', 'Date', 'Level', 'Title', 'YOE', 'TC', 'Base', 'Stock', 'Bonus', 'Details', 'Gender']\n",
    "','.join(cols)\n",
    "\n",
    "fpath = './data/ds-scraped.tsv'\n",
    "\n",
    "if not os.path.exists(fpath):\n",
    "    with open(fpath, 'w') as f:\n",
    "        f.write('\\t'.join(cols) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(parsed_rows):\n",
    "    with open(fpath, 'a', newline='', encoding='utf-8') as f:\n",
    "        tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "        tsv_writer.writerows(parsed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pg1 | Expand: 11.30 | Parse: 1.54 | Write: 0.72\n",
      "Pg2 | Expand: 11.24 | Parse: 1.60 | Write: 0.22\n",
      "Pg3 | Expand: 11.36 | Parse: 1.60 | Write: 0.22\n",
      "Pg4 | Expand: 11.26 | Parse: 1.62 | Write: 0.22\n",
      "Pg5 | Expand: 11.27 | Parse: 1.71 | Write: 0.22\n",
      "Pg6 | Expand: 11.20 | Parse: 1.49 | Write: 0.22\n",
      "Pg7 | Expand: 11.14 | Parse: 1.43 | Write: 0.22\n",
      "Pg8 | Expand: 11.19 | Parse: 1.48 | Write: 0.22\n",
      "Pg9 | Expand: 11.27 | Parse: 1.46 | Write: 0.22\n",
      "Pg10 | Expand: 11.29 | Parse: 1.47 | Write: 0.23\n",
      "Pg11 | Expand: 11.35 | Parse: 1.48 | Write: 0.22\n",
      "Pg12 | Expand: 11.27 | Parse: 1.47 | Write: 0.24\n",
      "Pg13 | Expand: 11.33 | Parse: 1.59 | Write: 0.22\n",
      "Pg14 | Expand: 11.31 | Parse: 1.47 | Write: 0.22\n",
      "Pg15 | Expand: 11.30 | Parse: 1.50 | Write: 0.22\n",
      "Pg16 | Expand: 11.26 | Parse: 1.47 | Write: 0.22\n",
      "Pg17 | Expand: 11.31 | Parse: 1.50 | Write: 0.22\n",
      "Pg18 | Expand: 11.30 | Parse: 1.46 | Write: 0.23\n",
      "Pg19 | Expand: 11.29 | Parse: 1.60 | Write: 0.22\n",
      "Pg20 | Expand: 11.29 | Parse: 1.58 | Write: 0.22\n",
      "Pg21 | Expand: 11.28 | Parse: 1.43 | Write: 0.23\n",
      "Pg22 | Expand: 11.31 | Parse: 1.63 | Write: 0.22\n",
      "Pg23 | Expand: 11.30 | Parse: 1.51 | Write: 0.24\n",
      "Pg24 | Expand: 11.29 | Parse: 1.52 | Write: 0.22\n",
      "Pg25 | Expand: 11.27 | Parse: 1.53 | Write: 0.24\n",
      "Pg26 | Expand: 11.27 | Parse: 1.44 | Write: 0.22\n",
      "Pg27 | Expand: 11.32 | Parse: 1.48 | Write: 0.22\n",
      "Pg28 | Expand: 11.30 | Parse: 1.48 | Write: 0.24\n",
      "Pg29 | Expand: 11.31 | Parse: 1.52 | Write: 0.23\n",
      "Pg30 | Expand: 11.30 | Parse: 1.46 | Write: 0.23\n",
      "Pg31 | Expand: 11.26 | Parse: 1.59 | Write: 0.24\n",
      "Pg32 | Expand: 11.31 | Parse: 1.64 | Write: 0.23\n",
      "Pg33 | Expand: 11.34 | Parse: 1.46 | Write: 0.23\n",
      "Pg34 | Expand: 11.41 | Parse: 1.49 | Write: 0.23\n",
      "Pg35 | Expand: 11.34 | Parse: 1.52 | Write: 0.23\n",
      "Pg36 | Expand: 11.27 | Parse: 1.46 | Write: 0.23\n",
      "Failed to click next page, retrying\n",
      "Pg37 | Expand: 11.36 | Parse: 1.43 | Write: 5.24\n",
      "Pg38 | Expand: 11.26 | Parse: 1.42 | Write: 0.24\n",
      "Pg39 | Expand: 11.34 | Parse: 1.63 | Write: 0.22\n",
      "Pg40 | Expand: 11.30 | Parse: 1.52 | Write: 0.22\n",
      "Pg41 | Expand: 11.26 | Parse: 1.47 | Write: 0.23\n",
      "Pg42 | Expand: 11.23 | Parse: 1.43 | Write: 0.24\n",
      "Pg43 | Expand: 11.30 | Parse: 1.52 | Write: 0.23\n",
      "Pg44 | Expand: 11.40 | Parse: 1.52 | Write: 0.22\n",
      "Pg45 | Expand: 11.42 | Parse: 1.45 | Write: 0.23\n",
      "Pg46 | Expand: 11.31 | Parse: 1.43 | Write: 0.22\n",
      "Pg47 | Expand: 11.35 | Parse: 1.39 | Write: 0.22\n",
      "Pg48 | Expand: 11.31 | Parse: 1.43 | Write: 0.22\n",
      "Pg49 | Expand: 11.31 | Parse: 1.37 | Write: 0.22\n",
      "Pg50 | Expand: 11.34 | Parse: 1.40 | Write: 0.22\n",
      "Pg51 | Expand: 11.27 | Parse: 1.39 | Write: 0.22\n",
      "Pg52 | Expand: 11.30 | Parse: 1.42 | Write: 0.22\n",
      "Pg53 | Expand: 11.36 | Parse: 1.36 | Write: 0.22\n",
      "Pg54 | Expand: 11.30 | Parse: 1.35 | Write: 0.22\n",
      "Pg55 | Expand: 11.32 | Parse: 1.40 | Write: 0.22\n",
      "Pg56 | Expand: 11.33 | Parse: 1.38 | Write: 0.22\n",
      "Pg57 | Expand: 11.31 | Parse: 1.40 | Write: 0.24\n",
      "Pg58 | Expand: 11.25 | Parse: 1.34 | Write: 0.22\n",
      "Pg59 | Expand: 11.25 | Parse: 1.51 | Write: 0.22\n",
      "Pg60 | Expand: 11.35 | Parse: 1.44 | Write: 0.23\n",
      "Pg61 | Expand: 11.32 | Parse: 1.43 | Write: 0.22\n",
      "Pg62 | Expand: 11.32 | Parse: 1.44 | Write: 0.22\n",
      "Pg63 | Expand: 11.33 | Parse: 1.45 | Write: 0.22\n",
      "Pg64 | Expand: 11.34 | Parse: 1.56 | Write: 0.22\n",
      "Pg65 | Expand: 11.32 | Parse: 1.46 | Write: 0.22\n",
      "Pg66 | Expand: 11.27 | Parse: 1.45 | Write: 0.23\n",
      "Pg67 | Expand: 11.26 | Parse: 1.39 | Write: 0.24\n",
      "Pg68 | Expand: 11.33 | Parse: 1.40 | Write: 0.22\n",
      "Pg69 | Expand: 11.29 | Parse: 1.40 | Write: 0.22\n",
      "Pg70 | Expand: 11.32 | Parse: 1.44 | Write: 0.22\n",
      "Pg71 | Expand: 11.31 | Parse: 1.55 | Write: 0.26\n",
      "Pg72 | Expand: 11.32 | Parse: 1.57 | Write: 0.22\n",
      "Pg73 | Expand: 11.35 | Parse: 1.55 | Write: 0.22\n",
      "Pg74 | Expand: 11.34 | Parse: 1.48 | Write: 0.22\n",
      "Pg75 | Expand: 11.39 | Parse: 1.46 | Write: 0.22\n",
      "Pg76 | Expand: 11.38 | Parse: 1.48 | Write: 0.22\n",
      "Pg77 | Expand: 11.34 | Parse: 1.54 | Write: 0.22\n",
      "Pg78 | Expand: 11.38 | Parse: 1.46 | Write: 0.22\n",
      "Pg79 | Expand: 11.35 | Parse: 1.51 | Write: 0.22\n",
      "Pg80 | Expand: 11.32 | Parse: 1.53 | Write: 0.22\n",
      "Pg81 | Expand: 11.31 | Parse: 1.50 | Write: 0.27\n",
      "Pg82 | Expand: 11.33 | Parse: 1.27 | Write: 0.22\n"
     ]
    }
   ],
   "source": [
    "pg = 0\n",
    "while not reached_end():\n",
    "    start = time.time()\n",
    "    \n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"#compTable\")\n",
    "    rows = table.find_element(By.CSS_SELECTOR, \"tbody\").find_elements(By.CSS_SELECTOR, \"tr\")\n",
    "    if len(rows) < 200:\n",
    "        expand_rows()\n",
    "    expand_time = time.time()\n",
    "\n",
    "    \n",
    "    rows = table.find_element(By.CSS_SELECTOR, \"tbody\").find_elements(By.CSS_SELECTOR, \"tr[data-has-detail-view]\")\n",
    "    parsed_rows = parse()\n",
    "    parse_time = time.time()\n",
    "    write_out(parsed_rows)\n",
    "    try:\n",
    "        next_page()\n",
    "    except:\n",
    "        print(\"Failed to click next page, retrying\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        next_page()\n",
    "    pg += 1\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    element = wait.until(EC.staleness_of(rows[0]))\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Pg{pg} | Expand: {expand_time - start:.2f} | Parse: {parse_time - expand_time:.2f} | Write: {end - parse_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_rows()\n",
    "parsed_rows = parse()\n",
    "write_out(parsed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Level</th>\n",
       "      <th>Title</th>\n",
       "      <th>YOE</th>\n",
       "      <th>TC</th>\n",
       "      <th>Base</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>Details</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Company, Location, Date, Level, Title, YOE, TC, Base, Stock, Bonus, Details, Gender]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/scraped.tsv', sep='\\t')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6b61cbd9fdb1c0111b6133c5c9bb1bf28d04207d27ade5cf5b9cec61bc0e8e3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
